{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aa3101",
   "metadata": {},
   "source": [
    "# GPT-3.5 Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d0936",
   "metadata": {},
   "source": [
    "## Arousal (with examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87ecba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stevenmesquiti/Desktop/working-with-lyle/Text-Annotation/blind-round-2\n"
     ]
    }
   ],
   "source": [
    "# set directory to get .env file. You will need to change this to wherever your API key (.env file) is, as well as where you want to save your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d522ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ardenspehar/Desktop/research-proj/research-proj\n"
     ]
    }
   ],
   "source": [
    "%cd \"/Users/ardenspehar/Desktop/research-proj/research-proj\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16067e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SID</th>\n",
       "      <th>value</th>\n",
       "      <th>arousal</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2609</td>\n",
       "      <td>s348</td>\n",
       "      <td>It's always goof to have moral support to trea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2369</td>\n",
       "      <td>s623</td>\n",
       "      <td>we should have responsible for keep our wilds ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1177</td>\n",
       "      <td>s622</td>\n",
       "      <td>A proposal to reduce the 28 country bloc's net...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1098</td>\n",
       "      <td>s496</td>\n",
       "      <td>to prevent the plants , its god's gift</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252</td>\n",
       "      <td>s083</td>\n",
       "      <td>As the climate changes, more heritage sites ar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3218</td>\n",
       "      <td>s062</td>\n",
       "      <td>With climate change currently happening and ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>634</td>\n",
       "      <td>s371</td>\n",
       "      <td>My sexual function may be getting worse, but n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2097</td>\n",
       "      <td>s153</td>\n",
       "      <td>Bananas may be healthier than the sports drink...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1152</td>\n",
       "      <td>s581</td>\n",
       "      <td>These vegan foods are good for health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1327</td>\n",
       "      <td>s195</td>\n",
       "      <td>Climate change is a scam and so are the people...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   SID                                              value  \\\n",
       "0        2609  s348  It's always goof to have moral support to trea...   \n",
       "1        2369  s623  we should have responsible for keep our wilds ...   \n",
       "2        1177  s622  A proposal to reduce the 28 country bloc's net...   \n",
       "3        1098  s496             to prevent the plants , its god's gift   \n",
       "4        1252  s083  As the climate changes, more heritage sites ar...   \n",
       "5        3218  s062  With climate change currently happening and ou...   \n",
       "6         634  s371  My sexual function may be getting worse, but n...   \n",
       "7        2097  s153  Bananas may be healthier than the sports drink...   \n",
       "8        1152  s581              These vegan foods are good for health   \n",
       "9        1327  s195  Climate change is a scam and so are the people...   \n",
       "\n",
       "   arousal  tone  \n",
       "0      NaN   NaN  \n",
       "1      NaN   NaN  \n",
       "2      NaN   NaN  \n",
       "3      NaN   NaN  \n",
       "4      NaN   NaN  \n",
       "5      NaN   NaN  \n",
       "6      NaN   NaN  \n",
       "7      NaN   NaN  \n",
       "8      NaN   NaN  \n",
       "9      NaN   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/ardenspehar/Desktop/research-proj/research-proj/master_text.csv') # load in the data using pandas and take a peek at the first 10 rows\n",
    "df.head(10) #grab the text column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91dbb214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Information about how to switch to a renewable...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seems to be a rundown of an interview with an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court blocks Exxon's attempt to stop the inves...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article is about weight loss based on diet...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Because I am a woman and sharing this article ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It matters to people I know, because they care...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I am very interested with this article so i fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This is primarily an attack on my home state, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We worry a lot about the future when it comes ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          text  rating\n",
       "text number                                                           \n",
       "1            Information about how to switch to a renewable...       1\n",
       "2            Seems to be a rundown of an interview with an ...       1\n",
       "3            Court blocks Exxon's attempt to stop the inves...       2\n",
       "4            The article is about weight loss based on diet...       2\n",
       "5            Because I am a woman and sharing this article ...       3\n",
       "6            It matters to people I know, because they care...       4\n",
       "7            I am very interested with this article so i fe...       4\n",
       "8            This is primarily an attack on my home state, ...       5\n",
       "9            We worry a lot about the future when it comes ...       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in your examples as `examples` and take a peek at the dataframe\n",
    "examples=pd.read_csv('/Users/ardenspehar/Desktop/research-proj/research-proj/arousal-rating-ex.csv', index_col=0)\n",
    "examples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8268145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df.value.values #save the text as an object called inputs. we'll use this to run our observations through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd35b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's always goof to have moral support to treat eating disorders\"\n",
      " 'we should have responsible for keep our wilds from fire free.'\n",
      " \"A proposal to reduce the 28 country bloc's net carbon emission failed\"\n",
      " \"to prevent the plants , its god's gift\"\n",
      " 'As the climate changes, more heritage sites are being affected.']\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 text values to check everything out \n",
    "print(inputs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef891bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load other dependencies \n",
    "import argparse \n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7645dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='BBPRIME'\n",
    "subtitle='Arousal'\n",
    "stim_set=title+'-'+subtitle\n",
    "seed=1 #set seed to make sure we get reproducable results \n",
    "temperature=0.0 #want a low baking temp to have little variability or creativity, and so we can get the same results every. single. time. range is 0-1\n",
    "engine='gpt-3.5-turbo' #change this to use different models available to you via OpenAI\n",
    "n_context=1\n",
    "cache = True   \n",
    "resume=False\n",
    "# MIDI='freq' #or 'name', 'number', 'freq'\n",
    "audience='People'\n",
    "item='Text'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "apiKey = os.environ.get('API_key') #for stevens key. update to you whatever you're calling your api_key. it should look like this in your file\n",
    "\n",
    "# apiKey = sk-ZJ...................4T\n",
    "\n",
    "openai.api_key = apiKey #installize the key so we can access\n",
    "\n",
    "if cache:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "996bcd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9732e02f424a6d83522f6563acb4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed\n",
      "Attempt 2 failed\n",
      "Attempt 3 failed\n",
      "Attempt 4 failed\n",
      "Attempt 5 failed\n",
      "Attempt 6 failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    138\u001b[0m (\n\u001b[1;32m    139\u001b[0m     deployment_id,\n\u001b[1;32m    140\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m     method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m choices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(choice\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices]\n\u001b[1;32m     78\u001b[0m cache[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m: choices,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mcreated\n\u001b[1;32m     82\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_prompt(current):#(example_idxs, current_pair):\n",
    "    prompt = f\"\"\"Your task is to classify a piece of as having either high or low arousal. Arousal is the extent to which a stimulus is calming or exciting. Examples of emotions with high\n",
    "arousal are anger, joy, fear, surprise, or anxiety. Examples of emotions with low arousal are sadness, relaxation, disgust, content, or peace.\n",
    "\n",
    "You will be provided with 2 examples of high arousal text and 2 examples of low arousal. Answer with only a whole number betwen 1 and 5, with 1 being\n",
    " low arousal and 5 being high arousal.\n",
    "\n",
    "\n",
    "Text: {examples.text.iloc[0]}  \n",
    "Rating: {examples.rating.iloc[0]}\n",
    "    \n",
    "Text: {examples.text.iloc[2]}\n",
    "Rating: {examples.rating.iloc[2]}\n",
    "\n",
    "Text: {examples.text.iloc[5]}\n",
    "Rating: {examples.rating.iloc[5]}\n",
    "\n",
    "Text: {examples.text.iloc[7]}\n",
    "Rating: {examples.rating.iloc[7]}\n",
    "    \n",
    "Text: {inputs[current]}\n",
    "Rating:\n",
    "\"\"\"\n",
    "    if n_context==0:\n",
    "        prompt = f\"\"\"What is the arousal of following piece of text? Answer only with a number between 1 and 5: 1 if low arousal, and 5 if high arousal Here is the text:\\n{inputs[current]}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "cache_folder = f'cache/{stim_set}'\n",
    "os.makedirs(cache_folder, exist_ok=True)\n",
    "def create_cache_filename():\n",
    "    filename = f'{stim_set}-{n_context}-{engine}-{temperature}-{seed}'\n",
    "    # if args.shuffle_context_each_draw:\n",
    "    #     filename += '-shuffle'\n",
    "    return os.path.join(cache_folder, filename + '.json')\n",
    "\n",
    "if not resume:\n",
    "    visited = []\n",
    "    predicted_ratings = np.zeros((len(inputs)))\n",
    "    request_count = 0\n",
    "\n",
    "cache = {}\n",
    "\n",
    "if cache and os.path.exists(create_cache_filename()):\n",
    "    cache = json.load(open(create_cache_filename()))\n",
    "cached_keys = list(cache.keys())\n",
    "\n",
    "for idx1, bname1 in enumerate(tqdm(inputs)):\n",
    "    current = idx1\n",
    "    if current in visited and predicted_ratings[current]!=0:\n",
    "        print(f'Already visited {current}')\n",
    "        continue\n",
    "\n",
    "    visited.append(current)\n",
    "\n",
    "    key = f'{current}'\n",
    "    if key in cached_keys:\n",
    "        choices = cache[key]['choices']\n",
    "        print('Using cached choices for key', key)\n",
    "    else:\n",
    "        prompt = generate_prompt(current)#(example_idxs, current_pair)\n",
    "        response=False\n",
    "        i=0\n",
    "        while not response:\n",
    "            i+=1\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=engine,\n",
    "                    messages=[{'role':'user', 'content':prompt}],\n",
    "                    temperature=temperature,\n",
    "                    timeout=10\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'Attempt {i} failed')\n",
    "                time.sleep(5)\n",
    "        choices = [dict(choice.items()) for choice in response.choices]\n",
    "\n",
    "        cache[key] = {\n",
    "            'prompt': prompt,\n",
    "            'choices': choices,\n",
    "            'created': response.created\n",
    "        }\n",
    "\n",
    "        request_count += 1\n",
    "        if cache and request_count % 5 == 0:\n",
    "            json.dump(cache, open(create_cache_filename(), 'w'))\n",
    "\n",
    "    try:\n",
    "        answer = choices[0]['message']['content'].replace('\\n', '').strip()\n",
    "        predicted_ratings[idx1] = int(answer)\n",
    "    except:\n",
    "        print('Error', cache[key])\n",
    "\n",
    "os.makedirs(f'predictions/{stim_set}', exist_ok=True)\n",
    "np.save(f'predictions/{stim_set}/{stim_set}-{n_context}-{engine}-{temperature}-{seed}.npy', predicted_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/ardenspehar/Desktop/research-proj/research-proj/master_text.csv\") #read in the OG dataframe again\n",
    "predicted_ratings=np.load(f'predictions/{stim_set}/{stim_set}-{n_context}-{engine}-{temperature}-{seed}.npy', allow_pickle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc227656",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings # just to check it ran smoothly\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f277f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arousal_df_gpt35_250=pd.DataFrame(predicted_ratings, columns=['gpt_arousal_ratings'])\n",
    "arousal_df_gpt35_250['value']=df.value.values #update to whatever the txt column is called\n",
    "arousal_df_gpt35_250['SID']=df.file_name.values #update to SID column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2397e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the work\n",
    "output_directory = '/Users/ardenspehar/Desktop/research-proj/research-proj/arousal_results/' #update to where you want to save the work\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "arousal_df_gpt35_250.to_csv(os.path.join(output_directory, f'{stim_set}-{n_context}-{engine}-{temperature}-{seed}.csv'), index=False) #saving the pandas dataframe to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b19c0",
   "metadata": {},
   "source": [
    "## Emotional Tone (with examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c073fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    # set directory to get .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ade05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ardenspehar/Desktop/research-proj/research-proj\n"
     ]
    }
   ],
   "source": [
    "# set directory to get .env file. You will need to change this to wherever your API key (.env file) is, as well as where you want to save your results\n",
    "%cd \"/Users/ardenspehar/Desktop/research-proj/research-proj\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68b2efe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SID</th>\n",
       "      <th>value</th>\n",
       "      <th>arousal</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2609</td>\n",
       "      <td>s348</td>\n",
       "      <td>It's always goof to have moral support to trea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2369</td>\n",
       "      <td>s623</td>\n",
       "      <td>we should have responsible for keep our wilds ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1177</td>\n",
       "      <td>s622</td>\n",
       "      <td>A proposal to reduce the 28 country bloc's net...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1098</td>\n",
       "      <td>s496</td>\n",
       "      <td>to prevent the plants , its god's gift</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252</td>\n",
       "      <td>s083</td>\n",
       "      <td>As the climate changes, more heritage sites ar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3218</td>\n",
       "      <td>s062</td>\n",
       "      <td>With climate change currently happening and ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>634</td>\n",
       "      <td>s371</td>\n",
       "      <td>My sexual function may be getting worse, but n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2097</td>\n",
       "      <td>s153</td>\n",
       "      <td>Bananas may be healthier than the sports drink...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1152</td>\n",
       "      <td>s581</td>\n",
       "      <td>These vegan foods are good for health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1327</td>\n",
       "      <td>s195</td>\n",
       "      <td>Climate change is a scam and so are the people...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   SID                                              value  \\\n",
       "0        2609  s348  It's always goof to have moral support to trea...   \n",
       "1        2369  s623  we should have responsible for keep our wilds ...   \n",
       "2        1177  s622  A proposal to reduce the 28 country bloc's net...   \n",
       "3        1098  s496             to prevent the plants , its god's gift   \n",
       "4        1252  s083  As the climate changes, more heritage sites ar...   \n",
       "5        3218  s062  With climate change currently happening and ou...   \n",
       "6         634  s371  My sexual function may be getting worse, but n...   \n",
       "7        2097  s153  Bananas may be healthier than the sports drink...   \n",
       "8        1152  s581              These vegan foods are good for health   \n",
       "9        1327  s195  Climate change is a scam and so are the people...   \n",
       "\n",
       "   arousal  tone  \n",
       "0      NaN   NaN  \n",
       "1      NaN   NaN  \n",
       "2      NaN   NaN  \n",
       "3      NaN   NaN  \n",
       "4      NaN   NaN  \n",
       "5      NaN   NaN  \n",
       "6      NaN   NaN  \n",
       "7      NaN   NaN  \n",
       "8      NaN   NaN  \n",
       "9      NaN   NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/ardenspehar/Desktop/research-proj/research-proj/master_text.csv') # load in the data using pandas and take a peek at the first 10 rows\n",
    "df.head(10) #grab the text column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbe650e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text number</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Consuming less is beneficial for earth conserv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I am a young person who is attempting to becom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>​​Runners are entering races that are above th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Spending less time working out for great resul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I used to drink a lot of carbonated beverages ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>It is warning people that climate change is re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Disaster funds are very slow to reach the peop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>If you take statins, beware of risk of staph s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text number                                               text  rating\n",
       "0            1  Consuming less is beneficial for earth conserv...       1\n",
       "1            2  I am a young person who is attempting to becom...       1\n",
       "2            3  ​​Runners are entering races that are above th...       1\n",
       "3            4  Spending less time working out for great resul...       1\n",
       "4            5  I used to drink a lot of carbonated beverages ...       0\n",
       "5            6  It is warning people that climate change is re...       0\n",
       "6            7  Disaster funds are very slow to reach the peop...       0\n",
       "7            8  If you take statins, beware of risk of staph s...       0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tone examples \n",
    "examples=pd.read_csv('/Users/ardenspehar/Desktop/research-proj/research-proj/tone-rating-ex.csv') #update with the relevant CSV path\n",
    "examples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e2d5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's always goof to have moral support to treat eating disorders\"\n",
      " 'we should have responsible for keep our wilds from fire free.'\n",
      " \"A proposal to reduce the 28 country bloc's net carbon emission failed\"]\n"
     ]
    }
   ],
   "source": [
    "inputs = df.value.values #save the text\n",
    "print(inputs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b35e7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load other dependencies \n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b414459",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='BBPRIME'\n",
    "subtitle='Arousal'\n",
    "stim_set=title+'-'+subtitle\n",
    "seed=1 #set seed to make sure we get reproducable results \n",
    "temperature=0.0 #want a low baking temp to have little variability or creativity, and so we can get the same results every. single. time. range is 0-1\n",
    "engine='gpt-3.5-turbo' #change this to use different models available to you via OpenAI\n",
    "n_context=1\n",
    "cache = True   \n",
    "resume=False\n",
    "# MIDI='freq' #or 'name', 'number', 'freq'\n",
    "audience='People'\n",
    "item='Text'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "apiKey = os.environ.get('API_key') #for stevens key. update to you whatever you're calling your api_key. it should look like this in your file\n",
    "\n",
    "# apiKey = sk-ZJ...................4T\n",
    "\n",
    "openai.api_key = apiKey #installize the key so we can access\n",
    "\n",
    "if cache:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(current):#(example_idxs, current_pair):\n",
    "    prompt = f\"\"\"Your task is to classify the emptional tone of a piece of text positive or negative. Emotional tone is the degree of positivity or negativity of a text. Positive tone has a more upbeat\n",
    "style and can include words like “good, well, new, love” but is not limited to these words. Negative tone reveals anxiety, sadness, and hostility and can include words like “bad, wrong, hate, too much” but is not limited to these words.\n",
    "You will be provided with 2 examples of text with a positive tone and 2 examples of text with a negative tone. Answer with only a number, with 0 being negative tone and 1 being positive tone\n",
    "\n",
    "Text: {examples.text.iloc[1]} \n",
    "Rating: {examples.rating.iloc[1]}\n",
    "    \n",
    "Text: {examples.text.iloc[2]}\n",
    "Rating: {examples.rating.iloc[2]}\n",
    "\n",
    "Text: {examples.text.iloc[4]}\n",
    "Rating: {examples.rating.iloc[4]}\n",
    "\n",
    "Text: {examples.text.iloc[5]}\n",
    "Rating: {examples.rating.iloc[]}\n",
    "    \n",
    "Text: {inputs[current]}\n",
    "Rating:\n",
    "\"\"\"\n",
    "    if n_context==0:\n",
    "        prompt = f\"\"\"Does this text have a positive or negative tone? Answer with only a number, with 0 being negative tone and 1 being positive tone. Here is the text:\\n{inputs[current]}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "cache_folder = f'cache/{stim_set}'\n",
    "os.makedirs(cache_folder, exist_ok=True)\n",
    "\n",
    "def create_cache_filename():\n",
    "    filename = f'{stim_set}-{n_context}-{engine}-{temperature}-{seed}'\n",
    "    # if args.shuffle_context_each_draw:\n",
    "    #     filename += '-shuffle'\n",
    "    return os.path.join(cache_folder, filename + '.json')\n",
    "\n",
    "if not resume:\n",
    "    visited = []\n",
    "    predicted_ratings = np.zeros((len(inputs)))\n",
    "    request_count = 0\n",
    "\n",
    "cache = {}\n",
    "\n",
    "if cache and os.path.exists(create_cache_filename()):\n",
    "    cache = json.load(open(create_cache_filename()))\n",
    "cached_keys = list(cache.keys())\n",
    "\n",
    "for idx1, bname1 in enumerate(tqdm(inputs)):\n",
    "    current = idx1\n",
    "    if current in visited and predicted_ratings[current] != 0:\n",
    "        print(f'Already visited {current}')\n",
    "        continue\n",
    "\n",
    "    visited.append(current)\n",
    "\n",
    "    key = f'{current}'\n",
    "    if key in cached_keys:\n",
    "        choices = cache[key]['choices']\n",
    "        print('Using cached choices for key', key)\n",
    "    else:\n",
    "        prompt = generate_prompt(current)\n",
    "        response = False\n",
    "        i = 0\n",
    "        while not response:\n",
    "            i += 1\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=engine,\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    temperature=temperature,\n",
    "                    timeout=10\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'Attempt {i} failed\\n{e}\\n')\n",
    "                \n",
    "                time.sleep(7) #7 second delay \n",
    "        choices = [dict(choice.items()) for choice in response.choices]\n",
    "\n",
    "        cache[key] = {\n",
    "            'prompt': prompt,\n",
    "            'choices': choices,\n",
    "            'created': response.created\n",
    "        }\n",
    "\n",
    "        request_count += 1\n",
    "        if cache and request_count % 5 == 0:\n",
    "            json.dump(cache, open(create_cache_filename(), 'w'))\n",
    "\n",
    "    try:\n",
    "        answer = choices[0]['message']['content'].replace('\\n', '').strip()\n",
    "        predicted_ratings[idx1] = int(answer)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}\\n\\n{cache[key]}')\n",
    "\n",
    "os.makedirs(f'predictions/{stim_set}', exist_ok=True)\n",
    "np.save(f'predictions/{stim_set}/{stim_set}-{n_context}-{engine}-{temperature}-{seed}.npy', predicted_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d581d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'predictions/BBPRIME-Arousal/BBPRIME-Arousal-1-gpt-3.5-turbo-0.0-1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ardenspehar/Desktop/research-proj/research-proj/master_text.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#read in the OG dataframe again\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_ratings\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstim_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstim_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_context\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mengine\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtemperature\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictions/BBPRIME-Arousal/BBPRIME-Arousal-1-gpt-3.5-turbo-0.0-1.npy'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/ardenspehar/Desktop/research-proj/research-proj/master_text.csv\") #read in the OG dataframe again\n",
    "predicted_ratings=np.load(f'predictions/{stim_set}/{stim_set}-{n_context}-{engine}-{temperature}-{seed}.npy', allow_pickle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be58ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'file_name', 'Text', 'year', 'month', 'gpt_coding_raw',\n",
       "       'gpt_coding_final', 'Tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings # just to check it ran smoothly\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_df_gpt35=pd.DataFrame(predicted_ratings, columns=['tone_ratings'])\n",
    "tone_df_gpt35['value']=df.value.values #update with text column \n",
    "tone_df_gpt35['SID']=df.file_name.values #update with indexing variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the work\n",
    "output_directory = '/Users/ardenspehar/Desktop/research-proj/research-proj/tone_results/' #update to your own directory \n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "tone_df_gpt35.to_csv(os.path.join(output_directory, f'{stim_set}-{n_context}-{engine}-{temperature}-{seed}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
